[TOC]







---

# 信息论



通信系统性能指标的评价

1. 有效性：用频谱复用程度（模拟系统）或频谱利用率（数字系统）来衡量。提高有效性的措施是，采用性能好的信源编码以压缩码率，采用频谱利用率高的调制减少传输带宽。
2. 可靠性：用输出信噪比（模拟系统）和传输错误率（数字系统）来衡量。提高可靠性的措施是，采用宽带调制以换取信噪比，采用高性能的信道编码以降低错误率。
3. 安全性：用信息加密强度来衡量。提高安全性的措施是，采用强度高的密码与信息隐藏或伪装方法。



---

## 离散信息的度量





#### 自信息和互信息



##### 自信息

> 表示该事件在发生前的不确定性，自信息大，预测的困难更大，不确定性大。必然发生的事无需预测，没有不确定性，所以自信息为0。
>
> 表示该事件在发生后提供(所包含)的信息量，即提供给信宿的信息量，即解除这种不确定性所需要的信息量。概率小的事件预测难度大，发生后提供的信息量大。

**自信息**: 事件集合$$X$$中的事件$$x=a_i$$的**自信息**定义为:
$$
I_X(a_i) = -\log P_X(a_i); 简记: I(x) = -\log p(x)\\
注: a_i \in A, \sum_{i=1}^n P_X(a_i) = 1, 0\le P_X(a_i) \le 1\\
$$

- 要求自信息I为非负值，所以对数的底必须大于1：以2为底：单位为比特 bit；以3为底：Tit；以e为底：奈特Nat；以10为底：迪特 Dit 或哈特 Hart
- 自信息为随机变量，$$I(x)$$是$$p(x)$$的单调递减函数，即概率大的事件自信息小，概率小的自信息大。

> 属于语法信息的层次，排除了语义和语用方面的含义



**联合自信息**: 联合事件集合$$XY$$中的事件$$x=a_i, y=b_j$$所包含的联合自信息定义为: 
$$
I_{XY}(a_i, b_j) = - \log P_{XY}(a_i, b_j); 简记: I(xy) = -\log p(xy)
$$

- $$p(xy)$$需要满足非负和归一化条件



**条件自信息**: 



##### 互信息

> 通过观察$$y$$获取关于$$x$$的信息。互信息为0表示两个事件毫无关联，互信息$$I(x;y) = I(x)$$(or$$I(y)$$)表示$$y$$的发生确定了$$x$$一定发生。**不满足非负性**。

离散随机**事件**$$x=a_i$$和$$y=b_j$$之间的**互信息**($$x\in X, y\in Y$$)定义为:
$$
I_{X;Y} (a_i; b_j) = \log \frac{P_{X|Y}(a_i | b_j) }{P_X(a_i)}, 简记 I(x;y) = \log \frac{p(x|y) }{p(x)}\\
I(x;y) = \log \frac{p(x|y) }{p(x)} = \log p(x|y) - \log p(x)  =  - \log p(x) - (- \log p(x|y) )\\
\therefore  I(x;y) = I(x) - I(x|y); x的信息量 - y发生时x的信息量
$$

- 互信息的单位与自信息单位相同，bit
- $$I(x)$$是$$x$$的不确定性，$$I(x|y)$$是$$y$$发生后$$x$$的不确定性，$$I(x) - I(x|y)$$为**不确定度消除**(减少)的部分，即$$y$$的发生得到的关于$$x$$的信息量。
- 互信息反映两个随机事件$$x$$和$$y$$之间的**统计关联度**。通信系统中，互信息的物理意义：信道输出端接收到某消息(或消息序列)$$y$$后，获得的关于输入端某消息(或消息序列)$$x$$的信息量。

互信息的性质：

1. **互易性**：$$I(x; y)=I(y; x)$$.($$\frac{p(x|y)} {p(x)} = \frac{p(y|x)} {p(y)} = \frac{p(xy)}{p(x)p(y)}$$)
2. 事件$$x,y$$**统计独立**时，**互信息为0**，即$$I(x; y) = 0$$.(e.g. 明天下雨与今天打了球的互信息为0)
3. 互信息**可正可负**. 正: 两事件的发生互相促进；一事件事件发生令另一事件更可能发生。负: 一事件事件发生令另一事件更不可能发生
4. 任何两个事件的**互信息不大于**其中任一事件的**自信息**。即$$I(x; y) \le I(x); I(x; y) \le I(y)$$



**条件互信息**: 联合事件集$$XYZ$$, 给定$$z \in Z$$条件下，$x(\in X)与y(\in Y)$之间的**条件互信息**: 
$$
I(x;y|z) = \log \frac{p(x|yz) }{p(x|z)}
$$





---

#### 信息熵概念与性质



##### 信息熵及性质

> 自信息的加权平均(权重为$$p(x)$$), 统计平均。平均互信息的特例。

**熵**: 离散随机变量$$X$$的**熵**定义为**自信息的平均值**
$$
H(X) = E_{p(x)}[I(x)] = \sum_x p(x)(-\log p(x)) = -\sum_x p(x)\log p(x)
$$


离散信息熵的性质：

1. 对称性：概率矢量$$\vec p = (p_1, p_2, …, p_n)$$中，各分量次序任意改变，熵不变。即$$H(p_1, p_2, …, p_n) = H(p_{j_1}, p_{j_2}, …, p_{j_n})$$
2. **非负性**：$$H(p_1, p_2, …, p_n) \ge 0$$，当某个$$p_j = 0$$时等式成立。仅对离散熵有效，对连续熵不成立。(对于连续随机变量，(差)熵不具有非负性)
3. 确定性：$$H(1, 0, …, 0) = 0$$，如果其中一个事件几乎必然出现，则是一个确知变量，不确定性为0
4. 扩展性：$$\lim_{\epsilon\to\infty} H_{n+1}(p_1, p_2, …, p_n) = H_n(p_1, p_2, …, p_n)$$，虽然极小概率事件的自信息大，但是在计算熵时，因为概率也很小，可以忽略。
5. 可加性：熵的可加性定理。熵的链原则。理解方式: ①复合事件的不确定性为组成该复合事件的各个简单事件集合的不确定性之和。②对事件输出直接测量所得信息量等于分成若干步测量所得信息量之和。③事件集合的平均不确定性可以分步解除，各步解除不确定性的和等于信息熵。
6. 极值性：**离散最大熵定理**：对于有限离散随机变量，当符号集中的**符号等概率发生**时，熵达到最大值。(仅适用于**有限**离散随机变量，对于无限可数符号集，只有附加其他约束条件最大熵才有意义)
7. 上凸性：$$H(\vec p) = H(p_1, p_2, …, p_n)$$是概率矢量$$\vec p$$的严格上凸函数。$$\vec p = \theta p_1 + (1-\theta)p_2 \Rightarrow H(\vec p) > \theta H(\vec p_1) + (1-\theta) H(\vec p_2)$$
8. 一一对应变换下的不变性：1. 离散随机变量(或矢量)经符号映射后的熵不大于原来的熵，仅当一一对应映射时熵不变。2. 离散随机序列经一一对应变换后，序列的熵不变，但单符号的熵可能改变。e.g. 极端情况: ①X的一个符号用Y的多个符号表示，如信源编码器。②X的多个符号表示Y的一个符号，如扩展源。



**熵的可加性**: $$H(XY) = H(X) + H(Y|X) = H(Y)+H(X|Y)$$. 

**熵的链原则**: 对$$N$$维随机矢量$$(X_1 X_2 … X_N)$$有: 
$$
H(X_1 X_2 … X_N) = H(X_1) + H(X_2|X_1) +...+H(X_N|X_{N-1}...X_1) \\
= \sum_{i=1}^N H(X_i|X_1 ...X_{i-1}) \le \sum_{i=1}^N H(X_{N})\\
X_1 X_2 … X_N统计独立时: H(X_1 X_2 … X_N) = H(X_1) + H(X_2)  + ...+ H(X_N). (强可加性)
$$



熵函数的唯一性：

1. 是概率的连续函数
2. 当各事件等概率时，是n(信源符号数)的增函数
3. 可加性






##### 联合熵&条件熵



##### 相对熵(KL散度)

> 相对熵 relative entropy，KL散度 Kullback-Leibler divergence (KLD)，Kullback_Leibler距离，信息散度 information divergence, 信息增益 information gain，鉴别信息，方向散度，交叉熵

若P和Q为定义在同一概率空间的两个概率测度，定义P相对于Q的相对熵为:(概率分布维数不限)
$$
D(P||Q) = \sum_x P(x)\log \frac{P(x)}{Q(x)}
$$

- 典型情况下，Q表示真实分布，P表示数据的理论分布、模型分布、Q的近似分布。$$D(P||Q) = \sum_x P(x)\log \frac{P(x)}{Q(x)} = \sum_x P(x)(\log P(x) - \log Q(x)) = \sum_x P(x)(I_Q(x) - I_P(X))$$ = 理论概率·(真实的信息量 - 理论的信息量) = 用理论概率加权(真实 - 理论信息量)。表示对真实分布Q的最优编玛比用理论分布P来进行编码，





##### Cross Entropy 交叉熵

- 度量两个概率分布间的差异性信息
- 理解：使用假设分布来编码时，应用在真实分布上的平均所需的bit数。用假设的分布$$Q$$来编码时，概率服从真实分布$$P$$时的熵。 故直观地 必有$$H(P,Q)\ge H(P)$$

$$
P: 真实分布; Q: 假设分布; 同一概率空间的两个概率测度\\
真实分布编码所需bit = P的熵 = H(p) = \sum_{i}p_i\times (-log(p_i))\\
P, Q交叉熵 = H(P, Q) = \sum _i p_i\times(- log(q_i))\\
P的熵H(P) + P相对于Q的相对熵D(P||Q) = PQ的交叉熵H(P, Q)\\
\sum _{i}p_i I_{p_i} +\sum _{i}p_i(I_{q_i} - I_{p_i}) =\sum _{i} p_i I_{p_i}; I_{p_i}为发生概率为p_i的事件的自信息\\
$$







---

###### 离散随机变量与事件之间的互信息

> 满足非负性

离散随机变量$$X$$与$$Y$$的某一取值$$y$$之间的互信息定义为:
$$
I(X;y) = \sum_x p(x|y)\log \frac{p(x|y)}{p(x)}
$$

- 由**$$y$$提供的关于$$X$$的信息量**。(注意是用条件概率做权)







---

##### 平均互信息

> 互信息的统计平均。在量纲上与熵对应。满足**非负性**。

离散随机变量X, Y之间的平均互信息定义为: 
$$
I(X; Y) = \sum_x p(x)I(Y; x) = \sum_x p(x) \sum_y p(y|x) \log \frac{p(y|x)}{p(y)}\\
 =  \sum_{x, y} p(x) p(y|x)\log \frac{p(y|x)}{\sum_x p(x)p(y|x)}\\
 = \sum_{i, j}p_i p_{ij}\log \frac{p_{ij}}{\sum_i p_i p_{ij}}; 其中p_i\triangleq p(x), p_{ij}\triangleq p(y|x)\\
平均互信息与熵之间的关系:\\
I(X; Y) = H(X) - H(X|Y)\\
I(X; Y) = I(Y; X) = H(Y) - H(Y|X)\\
I(X; Y) = H(X) + H(Y) - H(XY)
$$
![](http://op4fcrj8y.bkt.clouddn.com/18-7-5/42484927.jpg)

1. 非负性:  $$I(X; Y)\ge 0$$
2. 对称性: $$I(X; Y) = I(Y; X)$$
3. 凸函数性: $$I(X; Y)$$是概率分布p(x)的上凸函数。对于固定的概率分布p(x)，$$I(X; Y)$$为条件概率p(y|x)的下凸函数。
4. 极值性: $$I(X; Y)\le H(X); I(X; Y)\le H(Y);$$



###### 平均条件互信息



---

## 离散信源



#### 有限状态马尔可夫链



---

## 连续信息与连续信源







---

# 编码


$$
\left\{\begin{aligned}&概率匹配码:信源概率已知
\left\{\begin{aligned}
& 分组码\left\{\begin{aligned}
& 定长码\\
& 变长码
\end{aligned}\right. \\
& 非分组码: 码序列中的符号与信源序列中的符号无确定的对应关系
\\&(e.g. 算术编码)
\end{aligned}\right. \\

& 通用编码: (使用较广) 心愿符号概率未知
\end{aligned}\right.
$$





---

## 无失真信源编码

---

### 定长码





---

### 变长码







#### 变长码编码定理



---

### 最优编玛



#### 二元Huffman编码



#### 多元Huffman编码





---

## 离散信道及其容量



1. 离散无记忆信道(DMC): 信道的转移概率满足$$p(\vec y | \vec x) = \prod _{n=1} ^N p(y_n|x_n)$$. 数学模型为$$\{X, p(y_n|x_n), Y \}$$. 给定时刻的输出符号仅依赖于当前输入符号
2. 平稳(或恒参)信道: 
3. 单符号 离散信道: 



二元对称信道 Binary Symmetric Channel (BSC): 
$$
转移概率矩阵 P = 
\left[ \begin{aligned}
& 1-\varepsilon & \varepsilon \\
& \varepsilon & 1-\varepsilon\\
\end{aligned}\right]
$$








###### 信道容量定义

> 信道容量时信道的一种测量，传输信息能力的极限

单符号离散信道：

一个平稳离散无记忆信道的容量$$C$$定义为输入与输出之间平均互信息$$I(X;Y)$$的最大值:
$$
C\equiv \max_{p(x)}I(X;Y)
$$

- 信道容量的单位：bit/信道符号，Nat/信道符号。不引起混淆时可简写为bit或Nat
- 当信道给定后，$$p(y|x)$$固定，$$C$$仅与$$p(y|x)$$有关，与$$p(x)$$无关。信道转移概率是信道固有性质
- $$C$$是信道传输最大信息速率能力的度量

多维矢量信道

若$$X^N$$和$$Y^N$$分别为信道的$$N$$维输入与输出的随机矢量，则信道容量定义为:
$$
C\equiv \max_{p(x_1...x_N)}I(X^N;Y^N)\\
p(x_1...x_N)为信道输入矢量的概率
$$







---

### 单符号离散信道及其容量



#### 离散无噪信道的容量



1. 无损信道

每个输出符号只对应一个输入符号(一多对应)，即$$H(X|Y)=0$$，信道容量：

$$C=\max I(X;Y) = \max [H(X)-H(X|Y)] = \max H(X) = \log r$$

$$r$$为输入符号集的大小



2. 确定信道

多个输入符号对应一个输出符号(多一关系)，即$$H(Y|X)=0$$，信道容量：

$$C=\max I(X;Y) = \max [H(Y)-H(Y|X)] = \max H(Y) = \log s$$

$$s$$ 为输出符号集(字母表)大小



3. 无损确定信道



#### 离散对称信道的容量

对称信道：一个信道的转移概率矩阵按输出可分为若干子集，其中每个子集：每一行是其他行的置换，每一列是其他行的置换。
$$
e.g.P=\left[ \begin{matrix}
0.7 & 0.2 & 0.1 \\
0.1 & 0.2 & 0.7 \\
\end{matrix}\right]
可以分成两个子矩阵
\left[ \begin{matrix}
0.2\\0.2\\
\end{matrix}\right]
\left[ \begin{matrix}
0.7 & 0.1 \\
0.1 & 0.7 \\
\end{matrix}\right], 所以为对称信道\\

\left\{\begin{aligned}
&对称信道
\left\{\begin{aligned}
& 弱对称(准对称)信道: 需划分后满足\\
& 强对称: 不划分成子矩阵就满足: 每一行是其他行的置换，每一列是其他行的置换
\end{aligned}\right.\\
& 非对称信道
\end{aligned}\right.
$$


对于离散对称信道，当输入等概率时达到信道容量，信道容量为

$$C=H(Y) - H(p{11}, p{12}, ..., p_{1s})$$

$$H(Y)$$为输入等概率时信道输出的熵，$$p{11}, p{12}, ..., p_{1s}$$为转移矩阵第一行的元素。(实际上选择任一行均可，只是元素顺序不同，否则不可能为置换)

对强对称信道，输入等概率时达到信道容量，此时输出也等概率，信道容量为

$$C=\log s - H(p{11}, p{12}, ..., p_{1s});\  s 为输出符号集(字母表)大小$$







#### 一般离散信道的容量





---

### 级联信道及其容量





















---

# 信号与系统


$$
e^{jx} = \cos x + j\sin x\\
\cos x = \frac{1}{2}e^{jx} + \frac{1}{2}e^{-jx}\\
\sin x = \frac{1}{2j}e^{jx} - \frac{1}{2j}e^{-jx}\\
\cos \omega_0 t = \frac{1}{2}e^{j\omega_0 t} + \frac{1}{2}e^{-j\omega_0 t}\\
\sin \omega_0 t = \frac{1}{2j}e^{j\omega_0 t} - \frac{1}{2j}e^{-j\omega_0 t}
$$






## 线性时不变系统



### 离散时间线性时不变系统: 卷积和

离散时间单位脉冲序列的筛选性质:
$$
x[n] = \sum_{k=-\infty}^{\infty}x[k]\delta[n-k]
$$
单位脉冲(样本)序列响应: unit impulse(sample) response: h[n], 是线性时不变系统当输入为$$\delta[n]$$时的输出。故对离散时间LTI系统，有如下结果，称为卷积和convolution sum或叠加和superposition sum, 且等式右边称为x[n]和h[n]的卷积
$$
y[n] = \sum_{k=-\infty}^{\infty}x[k]h[n-k]\\
y[n] = x[n]*h[n]
$$
既然一个线性时不变系统对任何输入的响应都可以用系统对单位脉冲的响应来表示，那么线性时不变系统的单位脉冲响应就完全刻画了系统的特征。



### 连续时间线性时不变系统: 卷积积分





## 周期信号的傅里叶级数表示



### 连续时间周期信号的傅里叶级数表示

基波周期T: 满足x(t) = x(t+T)的最小非零正值T，而 $$\omega_0=2\pi $$/ T称为

周期连续时间信号的傅里叶级数表示：
$$
\text{Synthesis Formula: } x(t) = \sum_{k=-\infty}^{\infty}a_k e^{jk\omega_0 t} = \sum_{k=-\infty}^{\infty} a_k e^{jk(2\pi/T) t}\\
\text{Analysis Formula: } a_k = \frac{1}{T}\int_{T}x(t)e^{-jk\omega_0 t}dt = \frac{1}{T}\int_{T}x(t)e^{-jk(2\pi/T) t}dt\\
基波频率:\omega_0; 基波频率:T;\\
{a_k}:x(t)的傅里叶级数系数 \text{Fourier series coefficients}，频谱系数\text{Spectral coefficients}\\
直流或常数分量: a_0 = \frac{1}{T}\int_{T}x(t)dt;x(t)在一个周期内的平均值
$$








