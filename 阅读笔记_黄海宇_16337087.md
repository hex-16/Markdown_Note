[TOC]

-   author: 黄海宇, 16级信息安全, SDCS, SYSU, China. Time: 2018.6.17; Commit Time: 2018.6.24
-   student id: 16337087; email:huang_haiyu@outlook.com
-   笔记内容：对于 《Efficient Estimation of Word Representations in Vector Space》, 《Personal Recommendation Using Deep Recurrent Neural Networks in NetEase》, 《On Availability for Blockchain-Based Systems》三篇论文的读书笔记及阅读感悟、改进方案。同时是为加入In+ Lab的复杂网络（首选）或区块链的任务报告。

---

# 写在最前

-   由于第三篇论文耗时较长，且对论文所述内容很不了解，导致读书笔记迟交了，十分抱歉！（具体原因在第三篇论文的阅读感悟部分）
-   三篇论文的阅读顺序与笔记的记录顺序相同。
-   前两篇论文在阅读时有对相关性较大的知识做整理，放在background knowledge中，论文中有部分如果与background knowledge的相似，会直接整理在background knowledge的对应小标题下，同时第二篇论文的相关背景知识也有部分包含在第一篇论文的背景知识模块中。background knowledge部分可以直接跳过。
-   第三篇论文的改进方案写的非常简短，主要原因是自己对区块链相关知识十分不了解，短时间内还无法掌握区块链的基础知识，所以也不知道该如何去改进，十分抱歉！





---

# Efficient Estimation of Word Representations in Vector Space

-   2013年，google提出word2vec model







#### background knowledge

-   余弦相似度：测量两个**向量的夹角的余弦值**，以此作为它们之间的相似度。$${\text{similarity}}=\cos(\theta )={A\cdot B \over \|A\|\|B\|}={\frac  {\sum \limits _{{i=1}}^{{n}}{A_{i}\times B_{i}}}{{\sqrt  {\sum \limits _{{i=1}}^{{n}}{(A_{i})^{2}}}}\times {\sqrt  {\sum \limits _{{i=1}}^{{n}}{(B_{i})^{2}}}}}}$$
-   将符号形式的(英文 中文等)嵌入到一个数学空间里的方式，叫词嵌入（word embedding)。 Word2vec，是词嵌入（ word embedding)的一种
-   Latent Semantic Analysis (LSA) 潜在语义分析 以向量空间模型为基底的资讯检索技术，常以字词－文件矩阵表示字词与文件之间的关联；而其多以行代表字词〈term〉，列代表文件〈document〉。降维。
-   Latent Dirichlet Allocation (LDA).隐含狄利克雷分布 一种典型的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。降维。
-   epoch: 1个epoch表示过了1遍训练集中的所有样本。corpora n. 任何事物之主体；全集
-   鲁棒性 **Robustness** is the property of being strong and healthy in constitution.
-   tf-idf(**t**erm **f**requency–**i**nverse **d**ocument **f**requency): 加权技术。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降





###### One-Hot Encoding

>   独热编码

-   只有一个高位(1)，其余全是低位(0)，来表示一种状态。缺点是空间消耗大。

| Binary | Gray code | One-hot  |
| ------ | --------- | -------- |
| 000    | 000       | 00000001 |
| 001    | 001       | 00000010 |
| 010    | 011       | 00000100 |
| 011    | 010       | 00001000 |
| 100    | 110       | 00010000 |
| 101    | 111       | 00100000 |
| 110    | 101       | 01000000 |
| 111    | 100       | 10000000 |



###### Softmax

>   Softmax函数，归一化指数函数

-   Softmax将含任意实数的$$D$$维向量$$\vec z$$"压缩"到另一个$$D$$维实向量中$$\sigma(\vec z)$$，使得$$\sigma(\vec z)$$所有元素之和为1，从而可以成为一个概率分布。

$$
\vec z变换后的j维元素:\  \sigma ({\vec z} )_{j}={\frac {e^{z_{j}}}{\sum _{i=1}^{D}e^{z_{i}}}}\\
\sigma(\vec z) = [ \sigma ({\vec z} )_{1},  \sigma ({\vec z} )_{2}, ..., \sigma ({\vec z} )_{D}],\ \sum_{j=1}^{D} {\vec z}_{j} = 1
$$

-   实际上是有限项离散概率分布的梯度**对数归一化**(为什么说是梯度？)





###### Cross Entropy 交叉熵

-   度量两个概率分布间的差异性信息
-   理解：使用假设分布来编码时，应用在真实分布上的平均所需的bit数。用假设的分布$$Q$$来编码时，概率服从真实分布$$P$$时的熵。 故直观地 必有$$H(P,Q)\ge H(P)$$

$$
P: 真实分布; Q: 假设分布; 同一概率空间的两个概率测度\\
真实分布编码所需bit = P的熵 = H(p) = \sum_{i}p_i\times (-log(p_i))\\
P, Q交叉熵 = H(P, Q) = \sum _i p_i\times(- log(q_i))\\
P的熵H(P) + P相对于Q的相对熵D(P||Q) = PQ的交叉熵H(P, Q)\\
\sum _{i}p_i I_{p_i} +\sum _{i}p_i(I_{q_i} - I_{p_i}) =\sum _{i} p_i I_{p_i}; I_{p_i}为发生概率为p_i的事件的自信息\\
$$





###### *n*-gram models

-   n元语法模型：利用n个词，基于n-1阶马尔可夫链的概率语言模型，通过n个语词出现的概率推断语句结构。
-   n-gram model具有n-1阶马尔可夫性,当前单词与前面的n-1个单词有关系

In an *n*-gram model, the probability $${\displaystyle P(w_{1},\ldots ,w_{m})}$$ of observing the sentence $${\displaystyle w_{1},\ldots ,w_{m}}$$ is approximated as

$$
P(w_{1},\ldots ,w_{m})=\prod _{{i=1}}^{m}P(w_{i}\mid w_{1},\ldots ,w_{{i-1}})\approx \prod _{{i=1}}^{m}P(w_{i}\mid w_{{i-(n-1)}},\ldots ,w_{{i-1}})\\
The\ conditional\ probability\ can\ be\ calculated\ from\ n-gram\ model\ frequency\ counts:\\
P(w_{i}\mid w_{{i-(n-1)}},\ldots ,w_{{i-1}})={\frac  {{\mathrm  {count}}(w_{{i-(n-1)}},\ldots ,w_{{i-1}},w_{i})}{{\mathrm  {count}}(w_{{i-(n-1)},\ldots },w_{{i-1}})}}
$$



-   Unigram: an *n*-gram of size 1 is referred to as a "unigram". 对单个词进行分析，词出现概率与之前的输入输出无关。




###### Bag-of-words model

>   词袋模型

-   在NLP和信息检索(IR)下，对一个文本，忽略词序、文法，仅看作是一个词的集合，或说是词的组合，如同把词语装在一个袋子里。只关注在字典中已知的词。

e.g. 字典为 ["dennis", "sherry", "and", "love"], 文本"dennis love sherry and dennis." 的向量为[2, 1, 0, 1]




###### ANN & activation function

>   Artificial Neural Network ANN 人工神经网络，简称神经网络 Neural Network NN,类神经网络
>
>   激励函数 activation function： https://blog.csdn.net/hyman_yx/article/details/51789186 用来加入**非线性因素**以解决线性模型无法 解决的问题。激励函数例子：阀值函数（threshold function），大于某个值输出1（被激活了），小于等于则输出0（没有激活）。$$Sigmoid: f(x)=S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}, f^{'}(x) = f(x)(1-f(x))$$

典型神经网络有以下部分：
1. 结构 Architecture: 指定网络中的变量及其拓扑关系。\\e.g. 神经网络中的变量可以是神经元连接的权重(weights)和神经元的激励值(activities of the neurons)
2. 激励函数 Activity Rule: 一般激励函数依赖于网络中的权重（即该网络的参数）
3. 学习规则 Learning Rule: 指定网络中的权重如何随着时间推进而调整。This *learning* process typically amounts to modifying the weights and thresholds of the variables within the network


神经细胞层: 是一群彼此之间互不连接的神经元，它们仅跟其它神经细胞层有连接

![](http://op4fcrj8y.bkt.clouddn.com/18-6-18/25498743.jpg)

↑浅前馈神经网络、深神经网络



###### NNLM
>   Neural Network Language Model(NNLM) 神经网络语言模型

-   神经语言模型 Neural language models (or *Continuous space language models*) use continuous representations or embeddings(植入 埋藏) of words to make their predictions. These models make use of Neural networks. 维度灾难：Neural networks avoid this problem(维度灾难) by representing words in a distributed way, as non-linear combinations of weights in a neural net.(使用分布式的方式，例如神经网络中权重的非线性组合) The neural net architecture might be feed-forward(前馈 正向反馈，更简单) or recurrent(递归，更常见). 

依网络架构(Connectionism)分类：前馈神经网络，递归神经网络，强化式架构

- 前馈神经网络 FFNN feed-forward neural network: 最简单，各神经元分层排列，信息从前往后流动，每个神经元仅接收前一层的输入，并输出给下一层，各层没有反馈。基本神经网络神经元basic neural network cell，与其他神经元之间的连接具有权重，可以和前一层所有神经元有连接。给定一个数据集(输入+期望输出)，一般通过反向传播算法来训练前馈神经网络，属于监督式学习。
1. 输入层 input layer: 众多神经元（Neuron）接受大量非线形输入消息
2. 隐藏层 hidden layer(隐层): 一层或多层。通常，节点越多非线性越强(non-linear)，鲁棒性(robustness, 控制系统在一定结构、大小等的参数摄动下，维持某些性能的特性)更高。(typically, 1.2 - 1.5倍于input layer的Neuron)
3. 输出层 output layer(Softmax): 消息在神经元链接中传输、分析、权衡，形成输出结果


![](http://op4fcrj8y.bkt.clouddn.com/18-6-19/68478156.jpg)

前馈神经网络训练的基本步骤(监督式，反向传播 BP Back Propagation):

1.  寻找合适的数据，[输入数据，期望输出]数据对集合。
2.  向神经网络输入数据，输出模型输出，用模型输出的概率分布$$Q$$和期望输出的概率分布$$P$$，output layer使用softmax时，计算交叉熵$$H(P, Q)$$作为全局损失$$L$$。(也可用方差)
3.  利用梯度下降法的原理，更新网络参数向量$$\theta = (w_1, w_2,...,b_1, b_2...)$$,对于$$\theta$$的其中一个权重分量$$w$$的更新过程为: $$w_{new} = w_{old} - \eta\frac{\partial L}{\partial w}; \eta:learning \ rate$$ 
4.  重复2，3步直至$$\frac{\partial L}{\partial w}$$足够小，或达到训练次数的上限





###### RNNLM

- (时间)递归神经网络 RNN recurrent neural network: 神经元的输入信息 = 前一神经细胞层的输出 + 自身历史状态(具有短期记忆，n阶马尔可夫性???)。 梯度消失/指数级消失问题。


>   **递归神经网络 RNN**: 两种人工神经网络的总称。**时间递归神经网络 recurrent neural network，结构递归神经网络 **recursive neural network。时间递归神经网络的神经元间连接构成矩阵，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。RNN一般指代时间递归神经网络

- 循环神经元(Recurrent cells)在神经细胞层之间、时间轴上有连接(n阶马尔可夫性???)。每一个Recurrent cells内部保存自己的历史值。更新时具有额外的权重：与当前神经元之前值之间的权重和与同一神经细胞层各个神经元之间的权重(通常有)。性质：1. 维持一个特定的状态. 2. 如果不对其持续进行更新（输入），这个状态就会消失。每次调用激活函数时，把当前输入、其他权重(历史值 同层神经元之间的权重)作为输入，故历史信息会不断流失(影响比例降低)，通常4、5次迭代后历史信息几乎全部流失。(梯度/指数消失问题)


1.  input: 
2.  hidden: 主要是循环神经元，依权根据自身历史状态，当前输入，偏置，产生输出
3.  output: 






1.  feedforward neural network with a linear projection layer and a non-linear hidden layer
2.  NNLM that the word vectors are first learned using neural network with a single hidden layer



##### goal of this paper

使用大量的数据来学习高质量的词向量。简化以往模型的复杂度。

develop new model architectures that preserve the linear regularities among words -> maximize accuracy of the vector operations(...)





## Model Architectures

-   较LSA: 保持词的线性规律。较LDA: 在大型数据集上，计算开销更小。


模型训练复杂度正比于: $$O = E\times T\times Q$$；

1.  E: training epochs，过了一遍训练集中所有样本为一次(typically 3-50)；
2.  T: words in the training set: 训练集的总词数(billions)；
3.  Q: defined further for each model architecture；取决于模型结构





##### NNLM

>   https://blog.csdn.net/a635661820/article/details/44130285 Feedforward Neural Network Language Model(NNLM)原理及数学推导

-   probabilistic Feedforward Neural Network Language Model: 概率 前馈神经网络语言模型，神经网络概率语言模型[^1]。验证/预测一句话最可能的表达的语言模型。$$V$$: size of the vocabulary. 包含四层：
-   1.  input: 输入$$N$$个先前的词的one-hot
    2.  projection: 通过$$V\times D$$的矩阵(存了$$V$$个词的$$D$$维特征向量)映射成$$N$$个词对应的词向量，映射到 $$N \times D$$ shared projection matrix(dense, 稠密的); $$D$$: 词向量的维数(typically 50-200). 行：单词的特征向量。
    3.  hidden: hidden layer size $$H$$ is typically 500 to 1000 units(隐层神经元数量)，以映射层输出的$$N \times D$$矩阵为输入，$$H$$个神经元对输入矩阵进行运算。
    4.  output: 用softmax做归一化，保证概率和为1. dimensionality: $$V$$, 输出所有可能的词的概率分布，故维数 = size of the vocabulary($$V$$); 
-   $$Q = N\times D + N\times D\times H +H\times V$$
-   $$N: 历史词数; D: 词维数; H: Hidden\ layer\ size; V: 词典大小$$
-   原本复杂度集中在hidden和output之间的计算上($$H\times V$$)，但有多种方法可以避免。①使用hierarchical softmax ②训练时使用非标准化的模块以避免标准化。使用二叉树表示vocabulary，可以将输出单元数量减少到$$log_2(V)$$左右。因此，复杂度集中在$$N\times D\times H$$上。
-   论文models: 分层的softmax。vocabulary: Huffman binary tree, 导致基于哈弗满二叉树表示的词典的softmax复杂度仅需$$log_x(Unigram-perplexity(V))$$.




##### RNNLM

-   Recurrent Neural Net Language Model 递归神经网络语言模型。
-   1.  input:
    2.  hidden: dimensionality: $$H$$; 
    3.  output: 
-   a recurrent matrix: connects hidden layer to itself, using time-delayed connections.
-   $$Q = H\times H + H\times V$$
-   词维数($$D=H$$): 与hidden layer维数相同。与NNLM同理，$$H\times V$$可以下降到$$H\times log_2(V)$$, 复杂度主要来自$$H \times H$$






## New Log-linear Models

-   goal: minimize computational complexity. 从前面分析来看，复杂度主要集中在非线性的隐层。故去除隐层以加快计算。
-   神经网络语言模型 训练过程可以分解成：
-   1.  continuous word vectors are learned using simple model //计算词向量
    2.  the N-gram NNLM is trained on top of these distributed representations of words //训练N-gram NNLM




##### CBOW

-   连续词袋模型 Continuous Bag-of-Words Model：依据上下文预测一个词。输入：上下文$$N$$个词的one-hot，输出：输出词的one-hot。与前馈神经网络相比，去除非线性隐层，权重矩阵共享给所有词，词顺序不影响映射。三层模型，中间的是映射层。
-   $$Q= N\times D + D\times log_2(V)$$
-   过程: 上下文$$N$$个词的one-hot，维度为$$N\times V$$，每个词在词向量矩阵($$V\times D$$)"挑选出"一个$$1\times D$$，对$$N$$个词等权平均($$1/N$$)后得到映射层输出($$1\times D$$向量)，映射层输出作为输出层输入。输出层是一个Huffman树，非叶子节点是一个不代表词，代表某一类词的向量，叶子节点是代表一个词的词向量，根据能量函数对比选择左/右孩子，整个过程与不断累乘条件概率相似，最后得到一个概率最大的词向量。
-   continuous distributed representation of the context(unlike standard bag-of-words model)





#####  Skip-gram Model

-   continuous Skip-gram Model: 从一个词预测上下文。最大化使用同一句子的同位置的其他词。输入:当前词one-hot，输出:历史R个词和未来R个词one-hot，共输出$$C$$词($$2 \times R$$)。三层模型，中间的是映射层。
-   $$Q =C\times(D+D\times log_2(V))$$
-   过程：输入一个$$1\times V$$向量(词的one-hot)，与一个$$V\times D$$矩阵相乘，得到$$1\times D$$词向量，作为映射层输出。映射层输出作为输出层的输入，与输出层的Huffman树的每一个非叶子节点连接，计算出概率最大的C个词作为输出。
-   C: maximum distance of the words. For each training, select randomly R in [1, C], and use R words from history and R words from the future of the current word.






## Result

-   数据集分为语义、语法两个大类，共5+9个小类。没有输入任何词语形态学的信息，词语必须完全相同才算正确，即同义词也算错误。
-   维度数、训练数据量增加到一定程度后，对准确性的提升变小。大约为维度数300，训练单词391M后，出现边际效应(效果减小)，所以为了提升训练效果，应该同时提升词维数和训练数据量。
-   词维数为640时，使用相同大小的数据集，使用RNNLM, NNLM, CBOW, Skip-gram四种模型。RNN的表现最差，尤其是语义准确度9%。Skip-gram在语法上比CBOW略差，但在语义上优于CBOW，总体来说CBOW和Skip-gram最佳。
-   使用单CPU训练，使用全部字典词汇进行训练，Skip-gram用三天时间在语义和总体准确度上最佳。同时发现，用同等规模的不同数据集训练两次，效果与同等规模数据集训练三次相同甚至更优，故训练时应尽可能选择不同的数据集进行训练以提高训练效率。
-   使用大型并行训练发现，NNLM使用约9倍于CBOW的事件，约8倍于Skip-gram的训练时间，语义准确度、语法准确度和整体准确度都是最低的。


总而言之，CBOW和Skip-gram模型具有相当高的训练效率和预测的准确性。本paper开创的Word2vec模型为NLP提供了更高准确度，更低复杂度的解决方案，模型的应用前景十分广泛。但是感觉(自己感觉)，Word2vec模型有一种在简洁的路上走到了尽头的感觉，因为细想想不到什么可以显著降低复杂度的方法了。






## 阅读感悟

-   选择这篇论文作为阅读的第一篇的原因，一个是因为题目与首选方向契合，二是因为文件大小较小，再一个是因为是附件顺序的第一篇。原本以为文件大小较小，论文会比较容易阅读，结果发现刚一开篇，第一页就有一堆不知所云的名词，像neural networks, NLP这些都不知道是什么，所以一开始看了很多相关的知识。
-   论文中会说某某模型/方法复杂度高，耗时多，但是由于对这些模型/方法完全不懂，导致一开始进度十分缓慢，都是在恶补基础知识，从一个深的概念，阅读到一个不懂的词，然后再去了解这个词的含义，不断的看百科、论坛、博客等的解释。开始看这篇论文是在6月17日（周日）的晚上，看到文章第三个大标题New Log-linear Models的时候已经是6月18日晚上了，当然期间还不断的查阅前面涉及的知识。
-   这篇论文很多地方使用到的方法，或者提到的previous work都没有介绍其大致情况，为了大致了解这篇论文中，对引用到的东西的评价的原因，部分引用在google上查出了原论文大致阅读，有的就只是看了google的论文描述页内容。后来发现看引用到的论文是不太现实的（时间限制），因为有的论文特别长，例如引用13的Language Modeling for Speech Recognition of Czech，有91页之长，再加上阅读英语要慢很多，阅读完是一件难事。同时引用13有可能是信息出错，亦或是在google上找不到(知网和万方也没有)。（引用13的标题为Language Modeling for Speech Recognition in Czech，同时时间和作者也不同）
-   我对论文中将字典进行Huffman编码，将复杂度从线性降为log的方法感到惊叹，使用这一方法大大降低复杂度，惊叹的另一原因是论文中终于出现了一个自己学过的方法了，对这份熟悉感感到惊叹。对于Huffman编码这部分的算法，在网上查阅了资料，但最后也没搞懂具体是怎么实现的，短时间内学习大量知识还是有一定难度的。
-   对于文章提出的CBOW和Skip-gram模型的理解，是除了一开始了解一些相关的背景知识以外，耗时最多的部分，主要消耗在反向传播算法的数学推导和Huffman树相关的算法上，即便耗时多，也没能搞懂反向传播算法具体的推导和Huffman树上最优解的计算过程。只能定性的了解到，反向传播算法利用梯度下降法(或类似方法)，每次训练后将结果与预期结果比对，再与各个参数求一个相关的梯度，根据学习速度，向梯度的反方向(误差下降最快的方向)更新对应参数。而Huffman树上最优解的计算过程也只能定性的了解到，根据能量函数，选择左/右孩子，达到叶子节点后即为最优解，概率就是每一步的条件概率累乘。此外，网上找的对CBOW和Skip-gram模型的描述似乎有些在模型细节/实现上与论文所述的不太一样，导致自己的理解不断的被推倒重来，最后也不太确定自己的理解是否是正确的。
-   这篇论文是阅读的第一篇，也一定是耗时最多的一篇，因为写到感悟部分就已经是6月20日（周三）的深夜了，即便如此，还是有很多不明白不理解的地方，迫于原因，只能暂且放过，以大局为重了，同时以“这些和论文关联不大”来说服自己不再去盯着推导过程发呆。







## 改进方案

1.  random walk算法，局部极小值问题：梯度下降法或是在Result部分提到的mini-batch 异步梯度下降法，都无法保证能够获得全局最优解，只能获得局部最优解。这部分可以使用随机漫步(random walk)算法来求得全局最优解。
2.  多向量，词的多义性问题：不同的词语会有不同的词义，例如sound: You sound as if you’ve got a cold.(sound: 听起来) They return home sound and safe.(sound: 健全的)。该例中同一单词有不同的语义，如果使用同一词向量来计算，就会导致错误。比较简单的一个方法就是使用多个向量来表示一个词，在映射到词向量之前，如果是个有多个向量的词，就先对上下文进行运算，选择最匹配的向量，如果没有上下文，则选择出现可能性最高的向量。因此会多一个tag来表示出现的概率或直接表明哪个向量可能性最高，这样会导致复杂度变大。
3.  引入已知数据库加速训练：引入诸如牛津词典一类的词典里的数据作为已知数据，给同义词赋距离较近的词向量，反义词则反之。
4.  引入偏置向量加速训练：对于词性操作，可以采用与特定的长度较小偏执向量来计算，例如$$Vec[happy]-Vec[adj.]+Vec[adv.]=Vec[happily]$$;这样会导致一个词的不同形态，不同词性的词语，会彼此靠近，对于词性转换的操作也会方便很多。同时可能会有另一个好处，就是在已知词性或分析结果得到的期望词性与模型预测的词语不同时，可以在模型预测的词语的词向量附近，直接查找一个最近的，词性相符的词语作为预测结果。
5.  使用超图模型：可以使用超图模型，用超边表示词语间的关系，通过边的相似度匹配来预测词向量的位置。这样可以表达出多个词语之间的关系，弥补了论文模型只能一推多或多推一的不足。同时，使用超边来表达上下文词语间顺序的关系会更加方便，词语间的联系会更加受到重视，可能会有更好的效果。但是复杂度会高很多。



---

# Personal Recommendation Using Deep Recurrent Neural Networks in NetEase



-   网易基于深递归神经网络的个性化推荐（2016）。首次将RNN应用于电子商务个性化推荐。
-   主要介绍了基于当前浏览网页做实时预测的DRNN模型，DRNN基于遗传算法的自适应(automatic tuning)模型结构参数学习，基于历史浏览做预测的FNN(模拟CF)模型，DRNN与基于历史浏览做预测的FNN(模拟CF)的自适应协同预测的实现，及整个个性化推荐模型的训练过程和推荐准确度测试。


>   协同过滤算法Collaborative Filtering (CF): 利用用户与商品之间的关联性，利用具有相似购买经历或其他相同属性的用户，做出相同购买行为的概率较高的特点，做出概率预测。

-   协同过滤算法在用户遵循旧有购买模式时，有很高的预测精度。在一次购买会话开始时，协同过滤就已经根据用户属性（例如购买历史）做出了商品推荐了。但是用户并非每次都遵循自己既有的购买模式，有时购买行为是突发的（e.g. 例如一个男生恋爱了可能会突然买化妆品，这是根据以往记录无法预测的）。这就需要对用户当前的访问页进行分析，利用当前会话的信息进行个性化实时推荐。

-   根据当前浏览历史进行预测有诸多困难：

    1.  网页数量巨大：若每个状态用向量表示的网页的话，输入向量将会是十分巨大的，且状态组合将十分随机，学习过程复杂。
    2.  顺序与高效问题：网页浏览的顺序会表明用户购买商品的倾向性，即顺序敏感。同时个性化推荐需要高效，且会话的并发量会是百万级，故模型应该足够高效以缩短响应时间满足高并发。
    3.  数据集实时更新：训练集来自于考拉购物系统的访问日志，故训练集是实时更新的，所以模型需要相应的跟进进行训练，以反应用户新的购买模式。

-   为了解决以上三个问题，本论文使用了一个深递归神经网络模型(deep recurrent neural network DRNN)对用户的浏览模式建模，并进行实时个性化推荐。底层表示访问的网页，中间的隐层表示网页的组合与联系。与引用13《Training and analysing deep recurrent neural networks》相比（基础结构描述与《Efficient Estimation of Word Representations in Vector Space》笔记的background knowledge的RNNLM类似），有如下区别：
    1.  本文DRNN用于跟踪用户访问模式，直观上讲，目的是提供一个实时推荐以减小用户与用户想要的商品之间的距离。
    2.  常见的展开的RNN会使用滑动窗口的方式维护一个有限的状态，但是对于用户访问页来说，窗口太大会导致过大计算消耗，太小会导致准确度下降。所以在本文DRNN中，会将使用一个虚拟的历史状态以总结历史浏览。
    3.  使用FNN模拟CF算法，将FNN与DRNN合并生成预测结果，合并方式也是训练得到的。




##### background knowledge



###### overfitting

-   过拟合overfitting: 调试一个模型时，使用过多参数，太过受非法数据(噪声)影响，导致过度拟合当前模型，而不能适应更加一般的情况。
-   "the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably"
-   过拟合有关因素: 参数个数，训练数据集，模型结构

![](http://op4fcrj8y.bkt.clouddn.com/18-6-22/569954.jpg)

>   绿色线曲线(curve): overfitted model; black line: regularized model; 虽然绿色曲线更加适合当前数据集，但是对于未知的数据，绿色线可能有更高的误差。图源: wiki



###### SGD

>   Stochastic Gradient Descent 随机梯度下降法

-   每次从训练集中随机选择一个样本来进行学习，速度快，可以进行在线更新。
-   优化波动：更新时，可能不会按照正确的方向进行，会产生优化波动(扰动)。优化波动可能会导致原本方向为收敛向局部最优解，变成收敛向全局最优解。同时也会导致迭代次数增多，收敛速率变慢。





###### Genetic Algorithm(GA)

>   遗传算法 genetic algorithm (GA)

-   计算数学中用于解决最佳化的搜索算法，进化算法的一种。借鉴于进化生物学。
-   本质是高效，并行，全局搜索的方法，可以在搜索过程中自动获取和累积有关搜索空间的知识，并自适应地控制搜索过程以求得最佳解

算法大致过程描述:

1.  创建初始种群(一个染色体集合 chromosome set，每个chromosome由参数编码而成)
2.  loop: 繁殖下一代
    1.  计算每个个体的适应度fit
    2.  对个体进行fit排序
    3.  基因交叉
    4.  基因变异
3.  判断是否满足停止规则，不满足则重复<2>






## Overview Of Recommendation Module

-   考拉以往的推荐模块：考拉推荐系统以前使用协同过滤。历史购买记录作为CF算法的输入，CF算法输出商品推荐结果。这种离线过程：①推荐准确度低。②无法挖掘用户新的购买模式。
-   考拉推荐系统的运行过程：
    1.  DRNN: 用户向考拉网页服务器请求网页→访问日志以JSON文件保存在数据库→重新组织成session→作为DRNN输入→生成新的商品推荐。
    2.  FNN: 使用用户配置(历史浏览记录等)模仿CF算法生成商品推荐。
    3.  以一定方法合并DRNN和FNN的推荐结果，作为最终推荐结果，更新用户网页上对应的部件。
-   DRNN的输入：会话文件session document(由log document $$D_i^l$$组成): $$D_i^s = \{D_0^l, D_1^l, .., D_{k-1}^l\};$$//log文件存储用户请求的URL
-   时间戳:  $$ D_{j-1}^l.timestamp\le D_{j}^l.timestamp$$;
-   DRNN的输入可以简化成：$$D_i^s = \{p_0, p_1, .., p_{n-1}\};$$//$$p_i$$ : URL in $$D_j^l$$; 
-   当用M个新的session时，DRNN开始学习，M较大可以减少学习成本，M较小可以更快侦测到用户新的购买模式，M可调节。






## DRNN module

-   使用RNN模型是符合直观的，因为用户较短时间内访问的网页及其顺序可以很好的预测用户当时的购买行为，这种需要考虑短时间内的值的情况，自然适合可以保存短时数据的RNN模型。
-   一层隐层可以表示网页的访问顺序，而多层隐层可以表示网页访问的组合。




###### 单隐层的RNN

-   RNN用户处理序列信息，本文使用离散RNN，以时间戳标记各个状态。
-   如下方左图所示：x：输入，模型中为网页；y：输出，模型中为商品推荐的概率分布；a：隐层值；U：输入到隐层输入的矩阵；W：历史状态更新矩阵；V：隐层输出到输出的矩阵。

![](http://op4fcrj8y.bkt.clouddn.com/18-6-22/7080989.jpg)

-   $$a(i)$$: 状态i下，隐层的值: $$a(i)=f(Ux(i)+Wa(i-1)), f非线性; o(i)=softmax(Va(i))$$
-   关于基本RNN模型在《Efficient Estimation of Word Representations in Vector Space》笔记的background knowledge → RNNLM有更多描述。
-   RNN模型可以展开如上方的右图，每一个U, V, W都是相同的。





###### 本文的DRNN

-   本文使用带历史状态的三状态三隐层深度递归神经网络（外加一个历史状态）图示如下：

![](F:/中大 大二下/In+Lab/49938935.jpg)

$$
{\begin{aligned}
& M: 网页总数\\
& L: 隐层总数（图中L=3）\\
& N: 商品总数\\
& V_t: t状态访问的网页p_t的one-hot编码(1\times M 向量)\\
& E: 每一层的神经元数(对于每一层，神经元数目都可以单独调节,N<E<M)\\
& V_{out}: 商品购买可能性的概率分布(1\times N向量)\\
& W_i: 同层的状态(t-1)到状态状态(t)权\\
& b_i: 同一状态(t)的底下一层到该层的偏置\\
& Z_i: 同一状态(t)的底下一层的值与偏置之和到该层的权\\
& 隐层i层的值: a_i(t) = \left\{ \begin{array}{}
f(W_ia_i(t-1)+Z_i(a_{i-1}(t) + b_i(t))), i>1\\
f(W_ia_i(t-1)+Z_i(V_t+\theta(p_t))), i=1,隐层第一层\\
\end{array} \right.\\
& \theta(p_t) = 用户在网页p_t的停留时间\\
& 下标表示隐层第几层\\
\end{aligned}}
$$

-   维护一个大小为n(图示n=3)的滑动窗口，来保存用户访问的n个网页，当访问的网页大于n时，会有部分网页被挤出滑动窗口(30min后网页会被超时忽略)，超出的部分会被整合成一个历史状态，不直接产生输出，但是会影响滑动窗口中的状态的预测结果，如果用户当前访问的网页数$$x$$，有$$x\le n$$，则不会使用历史状态。

$$
历史状态输入: \bar V = \sum_{i=0}^{x-n}\epsilon _iV_i = 对超出滑动窗口的网页进行加权求和\\
时效因子(权) = \epsilon _i = \frac{\theta(p_i)}{\sum _{j=i}^{x-n}\theta(p_j)} = \frac{当前页停留时间}{从当前页开始计起所有历史页总停留时间}\\
$$

注意时效因子总和不一定为1，这里相当于RNN模型的梯度消失，越旧的信息影响力越小。历史状态只是过去的状态的近似表示，为的是保持预测精度的同时降低计算开销。



###### 与协同过滤算法协作

-   使用FNN模型模拟CF算法。

![](http://op4fcrj8y.bkt.clouddn.com/18-6-22/2851439.jpg)

上图为用于模拟CF的FNN模型。U个神经元，对应用户数。输入：用户是否购买过某个商品的0-1 bitmap($$1\times N$$)。输出：商品购买可能性的概率分布($$1\times N$$)。所用用户共享同一个FNN隐层。
$$
\bar E: 每层隐层的神经元数(每层都相同)\\
i层第j个神经元状态: \bar a_j^{(i)} = f(\sum_{x=0}^{\bar E -1} w_j^{(i-1)}\bar a_x^{i-1} + b_x^{(i-1)})
$$
FNN与DRNN的商品概率分布会被合并，且归一化为推荐商品概率分布，购买i商品的可能性：
$$
P(i) = 
\frac {f(\sum_{x=0}^{ E -1}( w_i^{L_0}a_{L_0}(t) + b_{L_0}(t)) +
\sum_{x=0}^{\bar E -1} (\bar w_i^{L_1}\bar a_x^{(L_1)} + b_x^{(L_1)}))}
{\sum_x f(\sum_{x=0}^{ E -1}( w_i^{L_0}a_{L_0}(t) + b_{L_0}(t)) +
\sum_{x=0}^{\bar E -1} (\bar w_i^{L_1}\bar a_x^{(L_1)} + b_x^{(L_1)}))}
$$
式中的权分别表示最后的隐层到输出层的权（采用SGD，随机梯度下降法）。RNN共有$$L_0$$层，FNN共有$$L_1$$层。



###### 实现细节

1.  生成训练数据

$$
{\begin{aligned}
& S = \{p_0, p_1, ... , p_{n-1} \}:用户会话\\
& p_i: 用户访问的第i个网页\\
& I: 用户会话S中购买的商品($$1\times N$$向量), 作为SGD对比的期望输出\\
& 训练样本(n>k(DRNN状态数))：
h, p_{n-k}, ..., p_{n-1} \rightarrow I\\
& h: 历史状态\\
& 训练目标: p_0 \rightarrow I;(最佳情况)...\\
& h, p_{n-k-1}, ..., p_{n-2} \rightarrow I\\
& 故实际上每个训练样本有n个规则进行训练\\
\end{aligned}}
$$




2.  DRNN实现


在Caffe上实现RNN模型，但由于Caffe 1.0不支持RNN，所以实际上使用的是同层共享权与偏置的卷积网络来模拟RNN。

下图是实际实现时，DRNN(左边四列)与FNN(最右一列)的模型图解

![](http://op4fcrj8y.bkt.clouddn.com/18-6-22/14574009.jpg)

vec: 访问的网页;
num: 实际购买结果($$I$$); 
k: 状态数，图中k=4

选择ReLU作为激励函数是因为在本实验中，拥有更高的收敛速率。且利于GPU运算。

顶层(loss layer)：使用softmax作为全局损失的计算函数loss function，来对比预测结果与期望的结果，并开始SGD算法以调整权与偏置。






## 模型自动优化

>   Ⅳ. MODEL OPTIMIZATIONS



###### 代码自动生成

-   由于Caffe中，更改参数需要改变脚本甚至需要重写，工作量巨大。因此实现了一个代码生成器，输入参数值的配置文件，生成对应模型的Caffe脚本。参数类型：
    1.  basic: 更改时，Caffe只需要重新设置值的参数
    2.  network structure: 更改时，Caffe的代码会完全改变。

$$
{\begin{aligned}
& w: 网络宽度。DRNN维护的状态数(不包括历史状态)。\\
& \qquad应略小于一个用户会话平均访问的网页，否则会出现过饱和现象而引入数据集噪声\\
& l: 网络长度。DRNN的层数，决定了DRNN的深度，不可太多否则可能有过饱和现象。\\
& h: 网络高度。the\ number\ of\ short\ links\ between\ the\ hidden\ layers\ and\ the\ loss\ layer\\
& \qquad 隐层与loss层之间的捷径数。增加捷径可以加大某个隐层神经元对结果的影响力。\\
\end{aligned}}
$$

生成Caffe代码的算法基本步骤:

1.  为所有状态创建神经层，创建神经层时，选择适当的层模板（类似于工厂模式的形式实现）填充基本参数后实例化。
2.  初始化loss层以外的历史状态。
3.  选择性的创建一些神经层到loss层的捷径。





###### 模型调整

-   使用遗传算法genetic algorithm(GA)作为启发式工具调整网络参数
-   匹配度$$ fit = accuracy + \frac{1}{1+loss}$$
-   chromosome: $$(w, l, h, a_1,...,a_L, ...其他基础参数...)$$；$$a_i：i$$层的神经元数；其他基础参数：学习速率，损失函数等。

算法大致过程描述:

1.  创建初始种群(一个染色体集合 chromosome set，每个chromosome由参数编码而成)
2.  loop: 繁殖下一代
    1.  计算每个个体的适应度 fit
    2.  对个体进行 fit 排序
    3.  基因**交叉**：染色体内基因（即参数）随机互换
    4.  基因**变异**：随机改变一些参数的值
3.  判断是否满足停止规则，不满足则重复<2>

-   使用GA算法可以获得复杂模型的足够好的局部最优解。







## EXPERIMENTS

-   训练数据集：training : validation : testing = 6 : 2 : 2; (validation为了避免过拟合)
-   准确度 = $$\frac{f(S)}{|S|}; |S|:训练样本数, f(S):$$正确预测的数量，仅在该会话有购买时才算。
-   epoch: 用所有训练样本训练并更新一次模型为一epoch。训练时，会有很多epoch。
-   ALS(alternating least squares 交替最小二乘)算法：一种混合协同过滤算法。因为用户当前购买行为与历史购买行为并无显著关系，故导致CF算法预测无效。故ALS算法的精确度是十分低的，带有历史状态的ALS预测精度也只有6%。



1.  Effect of Batching

-   无自适应：批量大会显著影响准确度，但同时也会导致高内存消耗。
-   自适应：批量大小不会显著影响准确度。



2.  Effect of FNN

-   无自适应：是否有FNN对准确度的影响不大，都在20%左右。
-   自适应：无FNN的准确度小幅上升，但仍然是20%左右。带有FNN的准确度增幅巨大(10%)

推荐系统实际使用中，会推荐k个商品，这会显著提升预测准确性（购买商品在k个中即算预测正确）。当$$k\in [1, 10]$$时：

-   无自适应：是否带FNN区别不大，带FNN在k=10时，准确度约48%
-   自适应：带FNN，k=10时，准确度大于50%达到约59%，准确度非常高。



3.  Effect of History State

-   无自适应：带历史状态的高于不带的10%。（状态数为4）
-   自适应：带或不带历史状态的准确度基本相同，均为30%左右。



4.  Convergence Rate

收敛速率主要取决于学习速率。不使用FNN可以减少收敛时间。默认初始学习速率$$\phi= 0.1$$ ，每100 epochs后，$$\phi= \frac{\phi}{5}$$，降低学习速率以避免模型在最优解附近波动。使用较大的学习速率可以减少收敛时间，但是由于可能收敛于次优解导致准确度降低。

-   初始学习速率为0.1时收敛速率最快，同为0.1时带有FNN慢于不带FNN。故猜测收敛时间是初始学习速率的下凸函数。



路径压缩比：$$\frac{L_2}{L_1}$$。$$L_1 = $$ 将商品加购前浏览的网页数。$$L_2 = $$ 正确预测商品时，浏览的网页数。

-   DRNN with FNN路径压缩比达到0.72，显著缩短用户与期望商品的距离。







## 阅读感悟

-   在开始看第一篇《Efficient Estimation of Word Representations in Vector Space》的时候，浏览过这篇文章的标题及概要，猜测主要会是第一篇的某个具体模型的细节与应用，也许有了第一篇的基础会稍微简单一些。但是实际读起来发现并不是这么一回事，仍旧十分苦难。
-   总的来说是比上一篇耗时少一点，原因是：即便仍然需要不断的查阅，补充背景知识、基础知识，但是本文对于引用的文章会有一个简要概括，需要去了解引用的文章讲述的大概内容的次数减少。
-   头一篇文章一开始是看电子版的，而这一篇开始是打印出来先看纸质版，将简要写在附近大大提高了review时的效率，同时对于整体的篇章结构认识更加清晰，通常往回翻看几眼就知道目前在看的部分在解决什么问题，目标是什么了。
-   同时与第一篇的阅读模式有一个很大的区别是：这一篇的阅读是第一遍完全纸质阅读，第二遍的时候才边review边写笔记的。最重要的原因是自己比较重视这篇论文，一是因为感兴趣，二是因为网易这个词就显得很亲切。
-   对于协同过滤算法，目前只是定性的知道其作用，感谢本文，让我知道原来生活中常常能接触到的不少东西都使用到了协同过滤算法，也就感觉这篇论文离生活十分近。
-   RNN模型在看第一篇的时候稍有了解，看到对于DRNN的描述的时候大体上也是懂的，但是到了DRNN具体实现的时候...同层没有互联？猜测是内积模拟的同层互联，但是paper中似乎并没有对内积部分做详细介绍，也不敢确定。
-   文中的DRNN是可以根据最新的session数据继续更新完善模型的，这一点十分钦佩。但是有一点，网页总数和商品总数是否可变呢？实际运行的时候网页总数和商品总数是会改变的，模型要如何适应这种变化呢？同时，即便总数不变，网页和商品暗示的信息也是可能改变的（例如某个在售的衣服变成了笔记本），这种改变如果大量出现是不是也会导致模型预测准确度受到影响？如果改变量较小也许会因为鲁棒性而几乎没有影响，而且也可以使用新的数据不断更新模型，应该不会出现我担心的问题吧。
-   我认为最精巧的部分是模型自适应。代码生成器部分还好，主要是模型调整部分，遗传算法的使用让我了解到了启发式算法，于是打开了新世界的大门。遗传算法部分仍然是定性大致了解，看了一些实现但仍然无法吃透，但不论怎么说，这种可能收敛到全局最优解的算法，适用的范围十分广泛，而且收敛速度较快，对于考拉商品推荐系统这种解空间较复杂的来说，尤甚。
-   总而言之，受到主观上较为感兴趣，客观上位于阅读顺序的第二篇，这篇论文是看的比较仔细的，也许因为第一篇的打击让我恶补了不少知识，尽管基本是浮于表面的理解，但至少没有看第一篇的那种绝望了。即便如此这篇的笔记也写到了六月22日（周五）深夜。
-   而且细看这篇文章会发现，也许因为作者国籍的原因，这篇文章语法上比较简单，叙述问题的逻辑比较考虑读者感受，从英语的角度来讲也是比第一篇好很多的。（有些部分讲的意思都差不多，客观上也导致了信息量较少）







## 改进方案

1.  参数回退：观察EXPERIMENTS部分的图可以发现，训练时，偶尔会有准确度大幅下降的情况出现，这样显然会对收敛速度产生影响。可以定期保存模型参数，在侦测到准确度下降幅度超过预定阈值时，回退到备份的模型参数，以减少为了抵消准确度下降导致的训练时间。
2.  噪音问题：用户购买行为有些时候本身就是无迹可寻的，例如：一男生原本在浏览新上市的笔记本电脑，而女朋友忽然提醒周年纪念日要到了，于是当机立断随便刷一下首页在某家店买了一套化妆品。这种购买模式不具有学习价值，因为系统无法获知具体用户的纪念日这样的隐私信息的，那么就需要对输入样本除噪。除噪的基本原理可以利用用户浏览页面与最终购买的商品之间的关联性，若关联性小于一定的阈值，就将这个样例剔除，不作为训练数据。
3.  剪枝与复杂度：感觉文中的推荐系统是比较复杂的（可能受到了上一篇简单得有点极端的影响），在大量用户访问的高并发的环境下，这种推荐系统是否可以在短时间内做出推荐预测呢？如果不行，可以仿照二叉决策树的剪枝算法的思想，在给DRNN提供输入之前进行一定预处理以去掉历史状态，将历史状态与滑动窗口的第一页合并作为滑动窗口的第一页的输入，同时预处理还可以用类似于第二点去噪声的原理，将可能为噪声的网页去除。
4.  基于向量的网页、商品合并：文中用网页总数与商品总数作为某些向量的维数，明显是计算量巨大的，可以考虑将内容差不多的网页进行合并，并维持在可接受的数量级。具体实现可以考虑上一篇论文的wrod2vec模型，分别生成网页、商品的向量，将近似的向量合并，推荐商品时，若推荐的是同一类商品，可以根据其他信息做推荐。例如基于商品特征的推荐算法，由于这种算法不需要根据用户会话实时即算，有较高的实时性，可以保证用户的交互体验。
5.  基于点距离的关联度推荐：文中的推荐系统很好的预测了用户一开始想要购买的东西，但是无法诱使用户买更多东西。想要有一个好的交易量，就要为用户推荐一些用户一开始可能没想买，但是看了有可能买的东西（例如我搜了电脑就可能会买一个鼠标）。商品之间的关联度可以采用多维空间里的点之间的距离来测得（实现是即为多维向量），经过适当学习，同类的商品的点会在空间中彼此相邻而形成点的聚合，做推荐时可以在空间中寻找另一个点聚合，插入到最后的推荐结果中。







---

# On Availability for Blockchain-Based Systems



-   POW：Proof of Work 工作证明。POW用在共识协议中。想要创建一个block的人，将交易打包成一个block，添加一个自己定义的随机整数，并计算hash sum。hash sum必须小于某个特定值（前一个区块的hash值）才会被网络接受。计算的难度在于，hash函数是密码安全的，除了暴力算法以外没有其他更好的解决方法，这就需要强大的算力来随机选择随机数以生成一个满足要求的hash sum，而这就需要投入计算能力。一旦矿工找到了满足目标的数，就会获得一个特定数额的资产作为奖励，这也是一种激励机制。而后矿工就可以在该区块上进行交易记录并发送到网络上，而后可以被其他节点接受并成为最新的区块链版本。另外，如果矿工允许，参与者可以给矿工自己选择的额外奖励。
-   fork：有可能出现两个矿工同时找到并发送一个新的区块，称为fork，出现fork则表明区块链不再保持一致性，参与者选择两者之一后必须等待下一个区块以重建一致性。解决fork的关键在于每一位矿工都必须从当前最长的链开始挖矿。著名的“51%攻击”是指，拥有超过网络上过半算力的矿工联合，可以通过强行在主链后添加一个包含伪造交易的区块，使得比真实的链长而导致其他节点以长的伪造的链为主链，而使得伪造交易获得网络承认。需要更改一个区块时，会导致后面的区块都需要进行重新计算（hash sum改变），这是区块链关键安全功能，比特币要求后面有6个区块时才可进行更改，以太币为12（含自身）.



-   orphans: 父交易未到，而子交易节点提早到达，从而产生孤儿交易。
    ![](http://op4fcrj8y.bkt.clouddn.com/18-6-24/30102645.jpg)






## COMMIT OF BITCOIN TRANSACTIONS

-   比特币传输时，将交易货币从一个表示源地址的数字传送到一个表示目的地址的数字上。如果矿工输出的hash sum小于输入的hash sum，则会因为矿工挖出了包含这一交易的区块，而给予矿工一定奖励。
-   由于各中原因，交易可能会出现延迟，而交易是需要按序到达的。交易会被矿池保留，如果输入交易未知，矿工会延迟新交易的内容，这就是orphan。





##### Data Collection Methodology

-   修改了比特币的btcd实现，并更改矿池以记录所有到来的交易，收集所有比特币交易并记录他们交付到区块链的时间。

检查矿池后发现，等待被包含的交易分为三类：

1.  通过测试，全部通过。
2.  交易被拒绝接受。
3.  orphaned 交易。

测试精确至秒，移除所有记录项副本（例如：父交易未到时，交易会被过滤掉，父交易已被学习后，才被考虑）。维护一个观察窗口，如果第一个区块到达后24小时之内到，则接受这一交易。



##### Inclusion Time of Bitcoin Transactions

实验一、二的主要差别在于开始收集的时间，实验一在2016.11.29，实验二在2017.04.13，其余实验参数基本一致，没有显著差别。每个实验收集约300000笔交易，两次实验结果相近，而其中差别表明实验二的orphans 比例更高。积压导致了额外的转发和排队。

实验结果对比说明：orphans 的交付时间远晚于直接被接受的交易。总体上，实验二的交付时间都要晚于实验一。对于实验二的orphans，试验结束后仍然有超过20%的orphans 没有成功交付。

除了失序到达会影响交付时间外，交易金额和锁定时间也有可能对交付时间产生影响。

对于交易金额(transaction fees)：当交易金额为0时，交易手续费（TX fee）为0。当交易金额为0时，实验二的交易费高许多，而实验一的直接提交与orphans交易没有区别，实验二的orphans金额反而更高。故较低的交易费用不太可能导致orphans的迟交。

对于锁定时间(locktimes)：由于日志记录器没有捕获到达矿池的区块的所有内容，故这一分析只能在以进入区块链的区块进行分析。绝大多数交易不包含锁定时间，实验二的含锁定期的比例高于实验一，也许是锁定期使用比例的增加，但是orphans在观察窗口之外从没有锁定时间。由于实验实际条件限制，无法知道观察期结束时，没有被加入区块链的orphans的锁定时间。最后的结论是，锁定时间不太可能会成为orphans延迟的决定性原因。

影响比特币交付时间还有很多其他无法进行控制的因素，例如比特币网络所依赖的因特网。





## COMMIT OF ETHEREUM TRANSACTIONS

##### Ethereum Transaction Handling

-   以太坊的gas相关概念借用自生活中汽车运输的油耗问题。交易如同开车在两地运送货物，而运输途中需要耗费汽油，消耗的汽油即以太坊中对矿工的奖励。
-   以太坊中，一个交易被提交到（虚拟分布式的）矿池后，①系统对交易进行公告让矿工开采，②新的区块被开采出来后将包含交易的区块加入链的某一分支上，③最后将交易纳入主链的区块里。
-   以太坊中，一个特定的交易是否最终交付或超时是不确定的，其次，某些状态无效的交易，在以后的状态也有可能变成有效的。即②不保证将交易加入区块链中，但某些情况下，交易有可能在一段时间后重新包含(③)。
-   交易包含在一个块中时，会被事先验证合法性，例如检查数字签名，交易序号，源账户是否有足够资金等参数是否合法。如果以太坊出现了分叉（两条同等高度的链），则首先挖出下一个区块的（长度更长）会被认为时合法的链，而另一条分叉的链的内容可能会被新产生的区块收留，成为叔块，而后其交易会重新进入交易池。交易在交易池时仍可能被删除。
-   如要高概率的确定一个交易被包含在链中，需要等待第一个包含的块后面有多个块被挖出，并且将包含交易的块作为祖先。每个后续的子孙块称为确认，当前版本在提交交易后要求有11个确认，则被认为交易被高概率留存在区块链中。





##### Data Collection and Basic Statistics & From First Inclusion to Commit

-   侦听收集交易的方式：修改客户机节点，为以太坊区块链编写了一个观察节点。允许客户端最多连接500个节点（默认25）以收集更多的交易，一般情况下与400多个节点通信。并且使得客户端在进行验证之前拦截交易公告，以避免交易公告被验证过程拦截而丢失。对于公告了的交易，记录本地时间戳，因为区块的时间戳是区块开始挖掘的时间而不是块生成的时间，且矿工的时间可能不同步。
-   由于以太坊的交易处理与区块时间与比特币不同，链有很大的分差可能，以太坊推荐在等待11个确认后，高概率的确认区块被包含在链中，如果是成为叔块保存在uncles的，会被重新返回到矿池。





##### 影响因素

1.  用户自定义的Gas Price and Limit: Gas Price对交付时间影响很大，高Gas Price下，长延迟的可能性很低，交付时间相对很短。而Gas Limit的影响则不确定，虽然有几个有过高的Gas Limit的交易延迟大，但是并未发现高Gas Limit与延迟之间的联系。
2.  网络延迟network delays: 以太坊在交易时，每个发送者的账户都不同，每个账户都维护一个序列号(nonce)，从而保证同一账户的交易能按序排列。先序抵达的交易会被放在矿池等待，直到其前一个交易抵达。实验表明网络延迟很可能对以太坊的交付时间产生影响。






## IMPACT OF THE BLOCK GAS LIMIT IN ETHEREUM

-   gas limit per block: 每个矿工网络的gas限制。如果该限制低于交易的gas required，则该交易不可能被交付。gas limit per block是为了避免Dos攻击。
-   gas limit per block过低时，可能导致协议的创立contract creation受到影响（影响最大）而无法交付，regular function calls to contracts的影响次之，但通常金融交易financial transfers不受到影响。





## TRANSACTION ABORT IN ETHEREUM

-   提出人工终止以太坊交易的机制。
-   竞争方式的中止：向网络发送相同nonce的交易，但是提高交易费transaction fee以提高后来的交易被接受的概率，后来的交易将目标账户设置为自己（付钱给自己），并把交易额transaction value设置为0。这会出现两种情况，一是原先的交易被接受，则满足一开始的预期，二是后来的交易被交付，则交易成功取消。
-   重发的方式重试：重新发布一个相同nonce的交易，并且会消耗更高transaction fee，由于新的交易的hash和数字签名与原先的不同，会被矿工的节点视为不同的新的交易。如果重新发送完全相同的交易，很可能因为与原先相同的原因而导致交易被挂起。



测试以上的中止方法，结果如下：

1.  在低于市场gas price的情况下，发送交易，大部分原先的交易就交付成功，16个超时的交易都成功中止。
2.  将超时时长设置为以太坊交付时长的均值后：超时中止的交易全部成功中止。部分原先交易先于为了中止而发出的交易，交付到区块链。可能原因：客户端准备终止交易时间过长；广播延迟等其他网络影响；矿池中交易的不合理安排。
3.  交易中止的区块被包含入区块链中的平均时间为45秒。







## 阅读感悟

-   开始阅读这篇论文的时候已经是六月23日（周六）中午了，由于周五睡得晚，这篇论文开始的也是相当的晚，主要原因是因为阅读顺序，而影响阅读顺序的主要原因是...。再加上周六下午去了游泳的25米达标测，时间上就更加紧张了。
-   由于对于区块链缺少基础，也缺少了解基础的时间，阅读到比特币交易的交付时间问题(Ⅲ)的时候，已经是周六晚上了，由于时间的紧迫，心里也更急了，越急就越想跳过一些冗长的基础知识部分，而一跳过一些基础部分就不理解后面的部分，又要倒回去补基础，而这一来就导致更加心急，以至于整篇文章字面意思不怎么懂，深层意思就更加不明白了。
-   区块链里面的一些概念和实际生活中的一些概念是比较相近的，例如以太坊中的gas相关概念，所以对其定性的理解是比较快的，但是真正含义，具体的说就是到实际应用场景的时候，相关的代码是怎么样的是完全不知道的。
-   作为CS专业的学生，区块链的相关科普看过不少，但不论哪一篇科普，看完了都还是不懂的，在阅读的过程中，终于首次明白了为什么挖矿是需要强大算力的了（可见相关基础可以说是没有的）。目前的理解是，对于比特币，需要算力去暴力猜测某个数，使得hash sum小于前一区块的hush sum。看了论文的一些描述，对于区块链的某些技术有恍然大悟的感觉，但是实际上对于区块链整体是怎么运作的，仍然是一头雾水的。
-   论文中一个关键词是 commit time，每当我看到这个词都十分的惭愧，因为按照HR发邮件的时间算一周的话，我是铁定要超时交付的了，而在看论文的过程中也不知不觉超过了当时看到邮件并回复的时间，就算按照这个时间也超过时间了，最后只能不断安慰自己要冷静，静下心来看完论文剩下的部分，查阅相关的知识慢慢搞懂。但事实上是真的慌了，即使容许自己拖延commit time到周日晚上，论文的内容也还是理解不了，阅读的速度和理解程度远不及前两篇论文的。







## 改进方案

1.  TRANSACTION ABORT IN ETHEREUM部分的实验中，每个实验的实验样本都比较少，容易受到偶然性的影响，为了提高可信度应增加实验样本容量。
2.  TRANSACTION ABORT：一些对时间不敏感的交易，可以适当增加重新发送交易的时间，并且在检测到交易成为叔块后，可以重置超时重新发送的计时器，因为成为叔块的交易会被放回到矿池，仍有机会加入到区块链中。
3.  比特币的交付时间的实验：实验中提到，由于lock time及矿池区块不包含所有信息的影响，在观察期内无法观察没有被加入区块链的orphans时间，可以在观察期结束后持续观察，更改客户端的实现，从而可以获知这部分在原本实验中无法获知的信息。


因为对区块链的内容十分不了解，改进方案只能写这么多了，十分抱歉！


[^1]: Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155, 2003